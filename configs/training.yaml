# Training configuration for K-Conditioned Decomposition

# K-conditioning parameters (core of the paper)
k_min: 2  # Minimum number of slots
k_max: 4  # Maximum number of slots (reduced from 6 for speed)
# During training, K is sampled uniformly from [k_min, k_max] for each batch
# This forces the model to learn decomposition at multiple granularities

# Optimization
learning_rate: 0.0001  # 1e-4, Adam default
weight_decay: 0.0  # No weight decay by default
max_epochs: 10  # Scientific validation with 10 epochs
gradient_clip: 1.0  # Clip gradients to prevent exploding gradients

# Loss weights
recon_loss_type: "mse"  # "mse" or "l1"
overlap_penalty_weight: 0.1  # Penalize overlapping alpha masks (VERY IMPORTANT)
tv_loss_weight: 0.01  # Total variation loss for smooth alphas
slot_usage_weight: 0.001  # Prevent dead slots / entropy

# Training settings
mixed_precision: true  # Use torch.cuda.amp for faster training
deterministic: false  # DISABLED for speed (cuDNN autotuner was too slow)
seed: 42  # Random seed for reproducibility

# Checkpointing
checkpoint_dir: "checkpoints"  # Directory to save checkpoints
log_dir: "logs"  # Directory for JSONL logs
save_every: 2  # Save checkpoint every 2 epochs
log_every: 50  # Log metrics every 50 steps (more frequent updates)

# Dataset (adjust these based on your data)
batch_size: 8  # Batch size (reduced for speed)
num_workers: 2  # Number of data loading workers (Kaggle has 2 cores)
